

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/favicon.ico">
  <link rel="icon" href="/favicon.ico">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="DNACore">
  <meta name="keywords" content="iOS Developer,iOS,iOS开发,macOS,macOS开发,macOS Developer,Android,安卓开发,Windows">
  
    <meta name="description" content="在 Mac&#x2F;Windows&#x2F;Linux 上免费本地部署 DeepSeek-R1 您是否正在寻找一种简单的方法来 离线安装 DeepSeek 或寻找 免费的 DeepSeek-R1 离线安装 ？在本地运行像DeepSeek-R1这样的强大模型已经成为开发人员、研究人员和 AI 爱好者的一大变革。大多数高级用户在本地运行 LLM 设置以完全控制数据和安全性，因此它也有助于 LL">
<meta property="og:type" content="article">
<meta property="og:title" content="在 Mac、Windows、Linux 上免费本地部署 DeepSeek-R1">
<meta property="og:url" content="https://dnacore.github.io/post/c8b6e8bc-ffb1-400f-ab85-5b28ae43e478.html">
<meta property="og:site_name" content="DNACore GitHub io">
<meta property="og:description" content="在 Mac&#x2F;Windows&#x2F;Linux 上免费本地部署 DeepSeek-R1 您是否正在寻找一种简单的方法来 离线安装 DeepSeek 或寻找 免费的 DeepSeek-R1 离线安装 ？在本地运行像DeepSeek-R1这样的强大模型已经成为开发人员、研究人员和 AI 爱好者的一大变革。大多数高级用户在本地运行 LLM 设置以完全控制数据和安全性，因此它也有助于 LL">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-02-07T08:36:00.000Z">
<meta property="article:modified_time" content="2025-02-07T11:18:12.021Z">
<meta property="article:author" content="DNACore">
<meta property="article:tag" content="DeepSeek">
<meta property="article:tag" content="DeepSeek-R1">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>在 Mac、Windows、Linux 上免费本地部署 DeepSeek-R1 - DNACore GitHub io</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dnacore.github.io","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null},"woyaola":null,"cnzz":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 50vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>DNACore GitHub io</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/background.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">在 Mac、Windows、Linux 上免费本地部署 DeepSeek-R1</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-02-07 16:36" pubdate>
          2025年2月7日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          22 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">在 Mac、Windows、Linux 上免费本地部署 DeepSeek-R1</h1>
            
            
              <div class="markdown-body">
                
                <!--
原始页面：
https://www.geeksforgeeks.org/how-to-run-deepseek-r1-locally-free-mac-windows-linux-guide/
-->

<p>在 Mac&#x2F;Windows&#x2F;Linux 上免费本地部署 DeepSeek-R1</p>
<p>您是否正在寻找一种简单的方法来 <em>离线安装 DeepSeek</em> 或寻找 <em>免费的 DeepSeek-R1 离线安装</em> ？<em><strong>在本地运行</strong></em>像<a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/deepseek-r1-rl-models-whats-new/"><em><strong>DeepSeek-R1</strong></em></a>这样的强大模型已经成为开发人员、研究人员和 AI 爱好者的一大变革。大多数高级用户在本地运行 LLM 设置以完全控制数据和安全性，因此它也有助于 LLM 充分发挥其潜力。本指南介绍了 在 Mac、Windows 或 Linux 上本地安装 DeepSeek-R1 的 <em><strong>四种行之有效的方法</strong></em> - 使用 Ollama 的简单性、Python 的灵活性、Docker 的可靠性或 llama.cpp 的优化。选择适合您的工作流程和硬件的方法。</p>
<p>[TOC]</p>
<h2 id="本地运行-DeepSeek-的优势"><a href="#本地运行-DeepSeek-的优势" class="headerlink" title="本地运行 DeepSeek 的优势"></a>本地运行 DeepSeek 的优势</h2><p>在本地运行<em><strong>DeepSeek</strong></em>具有多种优势，尤其是对于关注性能、隐私和控制的用户而言。以下是主要优势的细分：</p>
<table>
<thead>
<tr>
<th align="center"><em><strong>优势</strong></em></th>
<th align="center"><em><strong>描述</strong></em></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><em><strong>数据隐私与安全</strong></em></td>
<td align="center">完全控制您的数据，确保敏感信息保留在您的机器上，而无需第三方访问。</td>
</tr>
<tr>
<td align="center"><em><strong>离线功能</strong></em></td>
<td align="center">无需互联网连接即可运行，减少对云服务的依赖并确保偏远地区的可用性。</td>
</tr>
<tr>
<td align="center"><em><strong>定制与灵活性</strong></em></td>
<td align="center">能够微调模型、自定义设置并与本地应用程序或工作流程集成。</td>
</tr>
<tr>
<td align="center"><em><strong>性能和速度</strong></em></td>
<td align="center">由于延迟减少以及充分利用本地 CPU&#x2F;GPU 资源，响应时间更快。</td>
</tr>
<tr>
<td align="center"><em><strong>成本效益</strong></em></td>
<td align="center">避免云订阅费和 API 使用成本；扩展工作负载而无需额外费用。</td>
</tr>
<tr>
<td align="center"><em><strong>实验与开发</strong></em></td>
<td align="center">自由地进行实验、快速迭代并维护版本控制，不受外部限制。</td>
</tr>
<tr>
<td align="center"><em><strong>增强敏感应用程序的安全性</strong></em></td>
<td align="center">通过在安全、受控的环境中运行，非常适合具有严格监管要求的行业（例如医疗保健、金融）。</td>
</tr>
</tbody></table>
<h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a><em><strong>先决条件</strong></em></h2><ol>
<li><p><strong>所需操作系统</strong></p>
<ul>
<li>macOS（Intel 或 Apple Silicon）</li>
<li>Linux（x86_64 或 ARM64）| Ubuntu 24.04</li>
<li>Windows（通过 Windows Subsystem for Linux [WSL 2]）</li>
</ul>
</li>
<li><p><strong>所需硬件</strong></p>
<ul>
<li>最低 8GB 但 16GB+ RAM（建议用于最佳性能）。</li>
<li>10GB+免费存储空间。</li>
<li>兼容的 GPU（可选，但建议使用以便更快地进行推理）。</li>
</ul>
</li>
<li><p><strong>所需软件</strong></p>
<ul>
<li>终端访问（ WSL 中的 Windows cmd&#x2F;PowerShell）。</li>
<li><em><strong>基本工具</strong></em>：Python 3.10+、 <code>pip</code>和 <code>git</code>。</li>
</ul>
</li>
</ol>
<h2 id="DeepSeek-安装类型-比较以及哪一种最简单？"><a href="#DeepSeek-安装类型-比较以及哪一种最简单？" class="headerlink" title="DeepSeek 安装类型 - 比较以及哪一种最简单？"></a><em><strong>DeepSeek 安装类型 - 比较以及哪一种最简单？</strong></em></h2><p>用户可以使用四种方法免费在本地安装 DeepSeek-R1。对于大多数用户来说，Ollama 是最简单的方法，而 Python&#x2F;Hugging Face 则提供了最大的灵活性。以下是所有四种安装方法的详细比较：</p>
<table>
<thead>
<tr>
<th align="center"><em><strong>安装方法</strong></em></th>
<th align="center"><em><strong>易于安装</strong></em></th>
<th align="center"><em><strong>硬件</strong></em></th>
<th align="center"><em><strong>定制性</strong></em></th>
</tr>
</thead>
<tbody><tr>
<td align="center">Ollama</td>
<td align="center">⭐⭐⭐⭐⭐</td>
<td align="center">图形处理器&#x2F;中央处理器</td>
<td align="center">低</td>
</tr>
<tr>
<td align="center">Python</td>
<td align="center">⭐⭐⭐⭐</td>
<td align="center">图形处理器&#x2F;中央处理器</td>
<td align="center">高</td>
</tr>
<tr>
<td align="center">Docker</td>
<td align="center">⭐⭐⭐</td>
<td align="center">图形处理器&#x2F;中央处理器</td>
<td align="center">中等</td>
</tr>
<tr>
<td align="center">llama.cpp</td>
<td align="center">⭐⭐</td>
<td align="center">CPU&#x2F;GPU（慢）</td>
<td align="center">中等</td>
</tr>
</tbody></table>
<h2 id="如何使用-Ollama-在本地安装-DeepSeek-R1"><a href="#如何使用-Ollama-在本地安装-DeepSeek-R1" class="headerlink" title="如何使用 Ollama 在本地安装 DeepSeek-R1"></a>如何使用 Ollama 在本地安装 DeepSeek-R1</h2><h3 id="步骤1：安装Ollama"><a href="#步骤1：安装Ollama" class="headerlink" title="步骤1：安装Ollama"></a><em><strong>步骤1：安装Ollama</strong></em></h3><p>Ollama 简化了本地运行 LLM 的过程。请按照以下步骤进行安装：</p>
<h4 id="对于-macOS"><a href="#对于-macOS" class="headerlink" title="对于 macOS"></a><em><strong>对于 macOS</strong></em></h4><ol>
<li>访问 <a target="_blank" rel="noopener" href="https://ollama.ai/">Ollama.ai</a> 并下载 macOS 应用程序。</li>
<li>将 Ollama 图标拖到您的 <em><strong>应用程序</strong></em> 文件夹中。</li>
<li>打开应用程序，启动Ollama后台服务。</li>
</ol>
<h5 id="对于-Linux-Ubuntu-24-04-WSL（Windows）"><a href="#对于-Linux-Ubuntu-24-04-WSL（Windows）" class="headerlink" title="对于 Linux&#x2F; Ubuntu 24.04 &#x2F;WSL（Windows）"></a><em><strong>对于 Linux&#x2F;</strong></em> Ubuntu 24.04 <em><strong>&#x2F;WSL（Windows）</strong></em></h5><p>在终端中运行以下安装脚本：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">curl</span> -fsSL https://ollama.ai/install.sh | sh<br></code></pre></td></tr></table></figure>

<p>然后，启动 Ollama 服务</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ollama serve</span><br></code></pre></td></tr></table></figure>

<h5 id="验证-Ollama-安装"><a href="#验证-Ollama-安装" class="headerlink" title="验证 Ollama 安装"></a><em><strong>验证 Ollama 安装</strong></em></h5><p>检查 Ollama 是否已安装：</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">ollama <span class="hljs-comment">--version</span><br></code></pre></td></tr></table></figure>

<p>如果成功，您将看到版本号（例如 <code>ollama version 0.1.25</code>）。</p>
<h3 id="第-2-步：下载并安装-DeepSeek-R1"><a href="#第-2-步：下载并安装-DeepSeek-R1" class="headerlink" title="第 2 步：下载并安装 DeepSeek-R1"></a><em><strong>第 2 步：下载并安装 DeepSeek-R1</strong></em></h3><p>DeepSeek-R1 可能无法直接在 Ollama 的默认库中使用。请使用以下方法之一：</p>
<h4 id="方法-1：从-Ollama-提取（如果可用）"><a href="#方法-1：从-Ollama-提取（如果可用）" class="headerlink" title="方法 1：从 Ollama 提取（如果可用）"></a><em><strong>方法 1：从 Ollama 提取（如果可用）</strong></em></h4><p>首先，检查模型是否存在：</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ollama list</span><br></code></pre></td></tr></table></figure>

<p><em><strong>如果结果中列出了 deepseek</strong></em></p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ollama</span> pull deepseek-<span class="hljs-built_in">r1</span><br></code></pre></td></tr></table></figure>

<p><em><strong>在此过程中请耐心等待：</strong></em> 下载大型语言模型（可能为几 GB）需要稳定的互联网连接。下载时间将取决于您的互联网速度，连接越快，下载速度越快，而连接速度越慢，则可能需要几分钟或更长时间。</p>
<p>如果 <code>deepseek-r1</code> 没有列出，请使用 <em><strong>方法 2</strong></em>。</p>
<h4 id="方法-2：手动设置模型文件"><a href="#方法-2：手动设置模型文件" class="headerlink" title="方法 2：手动设置模型文件"></a><em><strong>方法 2：手动设置模型文件</strong></em></h4><p><em><strong>1.下载模型</strong></em></p>
<ul>
<li>从 <a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a> 或官方 DeepSeek 存储库等来源获取<em><strong>GGUF</strong></em>格式的 DeepSeek-R1 模型文件 （例如<code>deepseek-r1.Q4_K_M.gguf</code>）</li>
<li>将其保存到专用文件夹（例如 <code>~/models</code>）。</li>
</ul>
<p><em><strong>2. 创建模型文件</strong></em></p>
<p>在同一文件夹中，创建一个名为<code>Modelfile</code>的文件：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">FROM</span> ./deepseek-<span class="hljs-built_in">r1</span>.Q4_K_M.gguf<br></code></pre></td></tr></table></figure>

<p>将文件名替换为您的实际的 GGUF 文件。</p>
<p><em><strong>3. 建立模型</strong></em></p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ollama</span> create deepseek-<span class="hljs-built_in">r1</span> -f Modelfile<br></code></pre></td></tr></table></figure>

<h3 id="步骤3：运行DeepSeek-R1"><a href="#步骤3：运行DeepSeek-R1" class="headerlink" title="步骤3：运行DeepSeek-R1"></a><em><strong>步骤3：运行DeepSeek-R1</strong></em></h3><p>开始与模型聊天</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">ollama <span class="hljs-built_in">run</span> deepseek-r1<br></code></pre></td></tr></table></figure>

<p><strong>示例提示：</strong></p>
<figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">编写一个 Python 函数来计算斐波那契数。</span><br></code></pre></td></tr></table></figure>

<h3 id="步骤-4：验证安装（可选）"><a href="#步骤-4：验证安装（可选）" class="headerlink" title="步骤 4：验证安装（可选）"></a><em><strong>步骤 4：验证安装（可选）</strong></em></h3><p>确认模型处于活动状态：</p>
<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ollama list</span><br></code></pre></td></tr></table></figure>

<p>现在您将看到 <code>deepseek-r1</code> 列表。使用示例提示测试推理速度和响应质量。</p>
<h3 id="步骤-5：在-Web-UI-中运行-DeepSeek"><a href="#步骤-5：在-Web-UI-中运行-DeepSeek" class="headerlink" title="步骤 5：在 Web UI 中运行 DeepSeek"></a><em><strong>步骤 5：在 Web UI 中运行 DeepSeek</strong></em></h3><p>虽然 Ollama 提供与 DeepSeek 等模型的命令行交互，但基于 Web 的界面可以提供更简单、更用户友好的体验，就像您在 Web 浏览器上启动 DeepSeek 一样。Ollama Web UI 提供了这样的界面，简化了与 Ollama 模型交互和管理的过程。</p>
<blockquote>
<p><em><strong>注意：</strong></em> 对于不太熟悉命令行工具的用户，或者对于需要进行UI交互的任务，此图形界面尤其有用。</p>
</blockquote>
<h4 id="1-创建虚拟环境"><a href="#1-创建虚拟环境" class="headerlink" title="1.创建虚拟环境"></a>1.创建虚拟环境</h4><p>首先，创建一个虚拟环境，将您的 Python 依赖项与系统范围的 Python 安装隔离开来。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vim">sudo apt install <span class="hljs-keyword">python3</span>-venv<br><span class="hljs-keyword">python3</span> -<span class="hljs-keyword">m</span> venv ~/<span class="hljs-keyword">open</span>-webui-venv<br><span class="hljs-keyword">source</span> ~/<span class="hljs-keyword">open</span>-webui-venv/bin/activate<br></code></pre></td></tr></table></figure>

<h4 id="2-安装-Open-WebUI"><a href="#2-安装-Open-WebUI" class="headerlink" title="2. 安装 Open WebUI"></a>2. 安装 Open WebUI</h4><p>现在使用 pip 安装<em><strong>Open WebUI</strong></em></p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">pip install open-webui<br></code></pre></td></tr></table></figure>

<h4 id="3-启动服务器"><a href="#3-启动服务器" class="headerlink" title="3.启动服务器"></a>3.启动服务器</h4><p>安装 Open WebUI 后，使用以下命令启动服务器</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">open-webui serve<br></code></pre></td></tr></table></figure>

<p>打开您的网络浏览器并导航到 <em><strong><a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a></strong></em> - 您应该看到 Ollama Web UI 界面。</p>
<h3 id="DeepSeek-Ollama-故障排除提示"><a href="#DeepSeek-Ollama-故障排除提示" class="headerlink" title="DeepSeek Ollama 故障排除提示"></a>DeepSeek <em><strong>Ollama 故障排除提示</strong></em></h3><h3 id="1-未找到Model："><a href="#1-未找到Model：" class="headerlink" title="1. 未找到Model："></a><em><strong>1. 未找到Model</strong></em>：</h3><ul>
<li>确保模型名称正确或使用手动 GGUF 设置。</li>
<li>检查 <a target="_blank" rel="noopener" href="https://ollama.ai/library">Ollama 的模型库</a> 以查找替代的 DeepSeek 模型（例如 <code>deepseek-coder</code>）。</li>
</ul>
<h3 id="2-性能问题："><a href="#2-性能问题：" class="headerlink" title="2.性能问题："></a><em><strong>2.性能问题</strong></em>：</h3><ul>
<li>分配更多 RAM&#x2F;VRAM。</li>
<li>简化提示以便更快地做出响应。</li>
</ul>
<h3 id="3-WSL-错误："><a href="#3-WSL-错误：" class="headerlink" title="3. WSL 错误："></a><em><strong>3. WSL 错误</strong></em>：</h3><ul>
<li>更新 WSL <code>wsl --update</code>：。</li>
<li>重新启动 Ollama 服务。</li>
</ul>
<p>有关最新更新，请参阅：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://ollama.ai/">Ollama 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai">DeepSeek 官方仓库</a></li>
</ul>
<h2 id="通过-Python-和-Hugging-Face-安装并运行-DeepSeek"><a href="#通过-Python-和-Hugging-Face-安装并运行-DeepSeek" class="headerlink" title="通过 Python 和 Hugging Face 安装并运行 DeepSeek"></a><em><strong>通过 Python 和 Hugging Face 安装并运行 DeepSeek</strong></em></h2><p>通过 Python 和 Hugging Face Transformers 库与 DeepSeek 交互提供了一种强大而灵活的方法：</p>
<h3 id="步骤-1：安装依赖项"><a href="#步骤-1：安装依赖项" class="headerlink" title="步骤 1：安装依赖项"></a><em><strong>步骤 1：安装依赖项</strong></em></h3><p>首先，确保已安装 Python。然后，使用 pip 安装所需的库：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> torch transformers accelerate<br></code></pre></td></tr></table></figure>

<ul>
<li><em><strong><code>transformers</code>：</strong></em><em>提供对预先训练的模型以及使用它们的工具的访问。</em></li>
<li><em><strong><code>accelerate</code>：</strong></em><em>有助于优化模型执行，特别是对于更大的模型和 GPU。</em></li>
</ul>
<h3 id="第-2-步：下载模型"><a href="#第-2-步：下载模型" class="headerlink" title="第 2 步：下载模型"></a><em><strong>第 2 步：下载模型</strong></em></h3><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/models">在Hugging Face Model Hub</a><code>DeepSeek-R1</code> 上 查找 。</li>
<li>使用以下命令克隆存储库：</li>
</ul>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">https</span>://huggingface.co/deepseek-ai/deepseek-r1<br></code></pre></td></tr></table></figure>

<h3 id="步骤-3：运行推理"><a href="#步骤-3：运行推理" class="headerlink" title="步骤 3：运行推理"></a><em><strong>步骤 3：运行推理</strong></em></h3><p>创建 Python 脚本 <code>inference.py</code>：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-keyword">from</span> transformers import AutoModelForCausalLM, AutoTokenizer<br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;deepseek-ai/deepseek-r1&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;deepseek-ai/deepseek-r1&quot;</span>)<br>prompt = <span class="hljs-string">&quot;Explain quantum computing in simple terms.&quot;</span><br>inputs = tokenizer(prompt, <span class="hljs-attribute">return_tensors</span>=<span class="hljs-string">&quot;pt&quot;</span>)<br>outputs = model.generate(**inputs, <span class="hljs-attribute">max_length</span>=200)<br><span class="hljs-built_in">print</span>(tokenizer.decode(outputs[0]))<br></code></pre></td></tr></table></figure>

<h3 id="步骤-4：执行"><a href="#步骤-4：执行" class="headerlink" title="步骤 4：执行"></a><em><strong>步骤 4：执行</strong></em></h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> inference.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure>

<h3 id="通过-Python-和-Hugging-Face-进行-DeepSeek-故障排除技巧"><a href="#通过-Python-和-Hugging-Face-进行-DeepSeek-故障排除技巧" class="headerlink" title="通过 Python 和 Hugging Face 进行 DeepSeek 故障排除技巧"></a><em><strong>通过 Python 和 Hugging Face 进行 DeepSeek 故障排除技巧</strong></em></h3><ul>
<li><em><strong>内存不足错误</strong></em>：添加 <code>device_map=&quot;auto&quot;</code> 到 <code>from_pretrained()</code>。</li>
<li><em><strong>性能缓慢</strong></em>：使用 <code>quantization</code> （例如 <code>load_in_4bit=True</code>）。</li>
</ul>
<h2 id="通过-Docker-在本地安装-DeepSeek"><a href="#通过-Docker-在本地安装-DeepSeek" class="headerlink" title="通过 Docker 在本地安装 DeepSeek"></a><em><strong>通过 Docker 在本地安装 DeepSeek</strong></em></h2><p>使用 Docker 在本地运行 DeepSeek 提供了一个简单可靠的环境，消除了许多手动安装的复杂性：</p>
<h3 id="步骤-1：安装-Docker"><a href="#步骤-1：安装-Docker" class="headerlink" title="步骤 1：安装 Docker"></a><em><strong>步骤 1：安装 Docker</strong></em></h3><p>确保你的系统上安装了 Docker 和 Docker Compose。你可以从 Docker 官方网站 ( <a target="_blank" rel="noopener" href="https://www.docker.com/">https://www.docker.com/</a> ) 下载并安装它们。</p>
<ul>
<li><em><strong>Windows&#x2F;macOS</strong></em>：从 docker.com 下载 Docker Desktop</li>
<li><em><strong>Linux（</strong></em> Ubuntu &#x2F; Debian <em><strong>）</strong></em>：使用您的包管理器：</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sudo apt-<span class="hljs-built_in">get</span> update &amp;&amp; sudo apt-<span class="hljs-built_in">get</span> install docker.io<br></code></pre></td></tr></table></figure>

<h3 id="第-2-步：拉取-DeepSeek-Docker-镜像"><a href="#第-2-步：拉取-DeepSeek-Docker-镜像" class="headerlink" title="第 2 步：拉取 DeepSeek Docker 镜像"></a>第 2 步：拉取 DeepSeek Docker 镜像</h3><p>从注册表中提取官方图像（ <code>deepseek-image:tag</code> 用 DeepSeek 文档中的实际图像名称替换）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull deepseek/deepseek-llm:latest <span class="hljs-comment"># 示例镜像名称</span><br></code></pre></td></tr></table></figure>

<h3 id="步骤-3：运行-DeepSeek-容器"><a href="#步骤-3：运行-DeepSeek-容器" class="headerlink" title="步骤 3：运行 DeepSeek 容器"></a>步骤 3：运行 DeepSeek 容器</h3><p>使用适当的资源启动容器：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">docker</span> run -d --name deepseek-container -p <span class="hljs-number">8080</span>:<span class="hljs-number">8080</span> deepseek/deepseek:latest<br></code></pre></td></tr></table></figure>

<p>此命令以分离模式启动容器（-d），命名为<code>deepseek-container</code>，并将容器的端口 8080 映射到本地机器的端口 8080。</p>
<h3 id="步骤-4：验证安装"><a href="#步骤-4：验证安装" class="headerlink" title="步骤 4：验证安装"></a>步骤 4：验证安装</h3><p>检查容器是否正在运行：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">docker <span class="hljs-keyword">ps</span> -<span class="hljs-keyword">a</span> | <span class="hljs-keyword">grep</span> deepseek-container<br></code></pre></td></tr></table></figure>

<h3 id="步骤-5：与模型交互"><a href="#步骤-5：与模型交互" class="headerlink" title="步骤 5：与模型交互"></a>步骤 5：与模型交互</h3><p>通过API发送测试请求：</p>
<figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scilab">curl -X POST http:<span class="hljs-comment">//localhost:8000/v1/completions \</span><br>-H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \<br>-d <span class="hljs-string">&#x27;&#123;&quot;</span>prompt<span class="hljs-string">&quot;: &quot;</span>Hello, DeepSeek!<span class="hljs-string">&quot;, &quot;</span>max_tokens<span class="hljs-string">&quot;: 50&#125;&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="重要提示："><a href="#重要提示：" class="headerlink" title="重要提示："></a>重要提示：</h3><ol>
<li><p><em><strong>GPU 支持</strong></em> ：需要 NVIDIA 驱动程序和 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a></p>
</li>
<li><p><em><strong>模型权重</strong></em> ：某些模型需要单独下载权重。查看 DeepSeek 的文档。</p>
</li>
<li><p><strong>配置</strong> ：您可能需要为以下内容设置其他环境变量：</p>
<ul>
<li>模型参数</li>
<li>API 安全</li>
<li>资源分配</li>
</ul>
</li>
</ol>
<h2 id="使用-llama-cpp-手动设置-DeepSeek"><a href="#使用-llama-cpp-手动设置-DeepSeek" class="headerlink" title="使用 llama.cpp 手动设置 DeepSeek"></a><em><strong>使用 llama.cpp 手动设置 DeepSeek</strong></em></h2><p>仅适用于 CPU 或轻量级 GPU 使用。</p>
<h3 id="先决条件-1"><a href="#先决条件-1" class="headerlink" title="先决条件"></a><em><strong>先决条件</strong></em></h3><ul>
<li><p><strong>硬件</strong> :</p>
<ul>
<li>CPU：现代 x86-64 或 ARM（Apple Silicon）。</li>
<li>GPU（可选）：NVIDIA（CUDA）、AMD（ROCm）或 Apple Metal。</li>
</ul>
</li>
<li><p><em><strong>C++ 编译器</strong></em>：确保您已经安装了兼容的 C++ 编译器（例如<code>g++</code>）。</p>
</li>
<li><p><em><strong>CMake</strong></em>：构建所需<code>llama.cpp</code>。</p>
</li>
<li><p><em><strong>Git</strong></em>：克隆存储库。</p>
</li>
<li><p><em><strong>Python</strong></em>（可选）：如果需要，可以使用 Python 绑定。</p>
</li>
</ul>
<h3 id="步骤-1：安装或克隆-llama-cpp-存储库"><a href="#步骤-1：安装或克隆-llama-cpp-存储库" class="headerlink" title="步骤 1：安装或克隆 llama.cpp 存储库"></a><em><strong>步骤 1：安装或克隆 llama.cpp 存储库</strong></em></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/ggerganov/llama.cpp<br><span class="hljs-built_in">cd</span> llama.cpp &amp;&amp; make<br></code></pre></td></tr></table></figure>

<h3 id="步骤-2：构建-llama-cpp"><a href="#步骤-2：构建-llama-cpp" class="headerlink" title="步骤 2：构建 llama.cpp"></a><em><strong>步骤 2：构建 llama.cpp</strong></em></h3><h4 id="对于-Windows-（使用-CMake）"><a href="#对于-Windows-（使用-CMake）" class="headerlink" title="对于 Windows （使用 CMake）"></a><em><strong>对于 Windows</strong></em> （使用 CMake）</h4><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">mkdir build<br><span class="hljs-keyword">cd</span> build<br>cmake <span class="hljs-string">..</span><br>cmake <span class="hljs-params">--build</span> . <span class="hljs-params">--config</span> Release<br></code></pre></td></tr></table></figure>

<p>这将在目录中创建一个可执行文件<code>build/bin</code>。</p>
<h4 id="对于-Linux-macOS："><a href="#对于-Linux-macOS：" class="headerlink" title="对于 Linux&#x2F;macOS："></a><em><strong>对于 Linux&#x2F;macOS</strong></em>：</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">make clean &amp;&amp; make <span class="hljs-attribute">LLAMA_METAL</span>=1  # <span class="hljs-built_in">Enable</span> Metal <span class="hljs-keyword">for</span> Apple GPUs<br><span class="hljs-comment"># 或为 CUDA（NVIDIA GPU）：</span><br>make clean &amp;&amp; make <span class="hljs-attribute">LLAMA_CUBLAS</span>=1<br></code></pre></td></tr></table></figure>

<h3 id="步骤-3：下载-DeepSeek-GGUF-模型"><a href="#步骤-3：下载-DeepSeek-GGUF-模型" class="headerlink" title="步骤 3：下载 DeepSeek GGUF 模型"></a><em><strong>步骤 3：下载 DeepSeek GGUF 模型</strong></em></h3><h4 id="选项-1：从-Hugging-Face-下载预先转换的-GGUF-模型："><a href="#选项-1：从-Hugging-Face-下载预先转换的-GGUF-模型：" class="headerlink" title="选项 1：从 Hugging Face 下载预先转换的 GGUF 模型："></a><em><strong>选项 1</strong></em>：从 Hugging Face 下载预先转换的 GGUF 模型：</h4><ul>
<li>例如，在<a target="_blank" rel="noopener" href="https://huggingface.co/models?search=gguf">Hugging Face Hub</a><code>deepseek-gguf</code> 上 搜索 （根据您的模型版本进行调整）：</li>
</ul>
<h4 id="选项-2：自行将原始模型转换为-GGUF（高级）："><a href="#选项-2：自行将原始模型转换为-GGUF（高级）：" class="headerlink" title="选项 2：自行将原始模型转换为 GGUF（高级）："></a><em><strong>选项 2</strong></em>：自行将原始模型转换为 GGUF（高级）：</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"># 将 PyTorch/Safetensors 转换为 GGUF<br>python3 convert<span class="hljs-selector-class">.py</span> <span class="hljs-attr">--ctx-size</span> <span class="hljs-number">4096</span> <span class="hljs-attr">--outtype</span> f16 /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-model-dir<br># 量化模型（例如，<span class="hljs-number">4</span> 位的 Q4_K_M）：<br>./quantize /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-model<span class="hljs-selector-class">.gguf</span> /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-model-Q4_K_M<span class="hljs-selector-class">.gguf</span> Q4_K_M<br></code></pre></td></tr></table></figure>

<h3 id="步骤-4：运行模型"><a href="#步骤-4：运行模型" class="headerlink" title="步骤 4：运行模型"></a><em><strong>步骤 4：运行模型</strong></em></h3><p>使用 <code>main</code> 可执行文件与模型交互：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"># 对于 CPU<br>./<span class="hljs-selector-tag">main</span> -m /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-r1<span class="hljs-selector-class">.Q4_K_M</span><span class="hljs-selector-class">.gguf</span> -<span class="hljs-selector-tag">p</span> &quot;Hello, DeepSeek!&quot; -n <span class="hljs-number">512</span><br># 对于 GPU 加速（例如 NVIDIA CUDA）<br>./<span class="hljs-selector-tag">main</span> -m /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-r1<span class="hljs-selector-class">.Q4_K_M</span><span class="hljs-selector-class">.gguf</span> -<span class="hljs-selector-tag">p</span> &quot;Hello, DeepSeek!&quot; -n <span class="hljs-number">512</span> <span class="hljs-attr">--ngl</span> <span class="hljs-number">50</span><br></code></pre></td></tr></table></figure>

<h3 id="步骤-5：使用-API-服务器（可选）"><a href="#步骤-5：使用-API-服务器（可选）" class="headerlink" title="步骤 5：使用 API 服务器（可选）"></a><em><strong>步骤 5：使用 API 服务器（可选）</strong></em></h3><p>将模型作为与 OpenAI 兼容的 API 服务器运行：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">./server -m /<span class="hljs-selector-tag">path</span>/<span class="hljs-selector-tag">to</span>/deepseek-r1<span class="hljs-selector-class">.Q4_K_M</span><span class="hljs-selector-class">.gguf</span> <span class="hljs-attr">--port</span> <span class="hljs-number">8000</span> <span class="hljs-attr">--host</span> <span class="hljs-number">0.0</span>.<span class="hljs-number">0.0</span> <span class="hljs-attr">--ctx-size</span> <span class="hljs-number">4096</span><br></code></pre></td></tr></table></figure>

<p>通过发送请求 <code>curl</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/completions -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> -d <span class="hljs-string">&#x27;&#123;</span><br><span class="hljs-string">&quot;prompt&quot;: &quot;Explain AI alignment&quot;,</span><br><span class="hljs-string">&quot;max_tokens&quot;: 200</span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="使用-llama-cpp-进行-DeepSeek-设置故障排除技巧"><a href="#使用-llama-cpp-进行-DeepSeek-设置故障排除技巧" class="headerlink" title="使用 llama.cpp 进行 DeepSeek 设置故障排除技巧"></a><em><strong>使用 llama.cpp 进行 DeepSeek 设置故障排除技巧</strong></em></h3><ul>
<li><em><strong>模型兼容性</strong></em>：确保 DeepSeek 模型与 兼容<code>llama.cpp</code>。如果模型格式不同，则可能需要使用适当的工具转换模型。</li>
<li><em><strong>内存问题</strong></em>：如果遇到内存错误，请尝试使用模型的量化版本（<code>.ggml.q4_0</code>、<code>.gguf.q5_1</code>等）以减少资源使用。</li>
<li><em><strong>性能</strong></em>：为了获得更好的性能，请使用 GPU 加速（如果您的系统支持）。</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/DeepSeek/" class="print-no-link">#DeepSeek</a>
      
        <a href="/tags/DeepSeek-R1/" class="print-no-link">#DeepSeek-R1</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>在 Mac、Windows、Linux 上免费本地部署 DeepSeek-R1</div>
      <div>https://dnacore.github.io/post/c8b6e8bc-ffb1-400f-ab85-5b28ae43e478.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>DNACore</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年2月7日</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>更新于</div>
          <div>2025年2月7日</div>
        </div>
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/post/ee5d7735-158a-4e6e-b435-4f5fa89756fd.html" title="手把手教你压住硬盘的C1、E1值-APM控制">
                        <span class="hidden-mobile">手把手教你压住硬盘的C1、E1值-APM控制</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="lv-container" data-id="city" data-uid="MTAyMC8yODU3Ni81MTQ3">
    <script type="text/javascript">
      Fluid.utils.loadComments('#lv-container', function() {
        Fluid.utils.createScript('https://cdn-city.livere.com/js/embed.dist.js');
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
